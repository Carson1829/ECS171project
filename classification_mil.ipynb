{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv(\"./data/cleaned_data_mil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data = tracks.copy()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "track_data['genre'] = encoder.fit_transform(track_data['genre'])\n",
    "\n",
    "features = track_data[['popularity', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', \n",
    "                       'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                       'time_signature']]\n",
    "target = track_data['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_list = [5, 10, 20, 50, 100]\n",
    "\n",
    "results = {\n",
    "    'n_neighbors': [],\n",
    "    'accuracy': [],\n",
    "    'precision_micro': [],\n",
    "    'recall_micro': [],\n",
    "    'f1_micro': [],\n",
    "    'precision_macro': [],\n",
    "    'recall_macro': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "for n in n_neighbors_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "    results['n_neighbors'].append(n)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision_micro'].append(precision_micro)\n",
    "    results['recall_micro'].append(recall_micro)\n",
    "    results['f1_micro'].append(f1_micro)\n",
    "    results['precision_macro'].append(precision_macro)\n",
    "    results['recall_macro'].append(recall_macro)\n",
    "    results['f1_macro'].append(f1_macro)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axs[0, 0].plot(results['n_neighbors'], results['accuracy'], marker='o')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 0].set_xlabel('n_neighbors')\n",
    "axs[0, 0].set_ylabel('Accuracy')\n",
    "\n",
    "axs[0, 1].plot(results['n_neighbors'], results['precision_micro'], marker='o')\n",
    "axs[0, 1].set_title('Precision (Micro)')\n",
    "axs[0, 1].set_xlabel('n_neighbors')\n",
    "axs[0, 1].set_ylabel('Precision (Micro)')\n",
    "\n",
    "axs[0, 2].plot(results['n_neighbors'], results['recall_micro'], marker='o')\n",
    "axs[0, 2].set_title('Recall (Micro)')\n",
    "axs[0, 2].set_xlabel('n_neighbors')\n",
    "axs[0, 2].set_ylabel('Recall (Micro)')\n",
    "\n",
    "axs[1, 0].plot(results['n_neighbors'], results['f1_micro'], marker='o')\n",
    "axs[1, 0].set_title('F1 Score (Micro)')\n",
    "axs[1, 0].set_xlabel('n_neighbors')\n",
    "axs[1, 0].set_ylabel('F1 Score (Micro)')\n",
    "\n",
    "axs[1, 1].plot(results['n_neighbors'], results['precision_macro'], marker='o')\n",
    "axs[1, 1].set_title('Precision (Macro)')\n",
    "axs[1, 1].set_xlabel('n_neighbors')\n",
    "axs[1, 1].set_ylabel('Precision (Macro)')\n",
    "\n",
    "axs[1, 2].plot(results['n_neighbors'], results['recall_macro'], marker='o')\n",
    "axs[1, 2].set_title('Recall (Macro)')\n",
    "axs[1, 2].set_xlabel('n_neighbors')\n",
    "axs[1, 2].set_ylabel('Recall (Macro)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./result/KNN_Model_Evaluation_Results.txt', sep='\\t', index=False)\n",
    "\n",
    "def save_plot(y_values, title, ylabel, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(results['n_neighbors'], y_values, marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('n_neighbors')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./result/{filename}')\n",
    "    plt.close()\n",
    "\n",
    "save_plot(results['accuracy'], 'Accuracy', 'Accuracy', 'KNN_Accuracy.png')\n",
    "save_plot(results['precision_micro'], 'Precision (Micro)', 'Precision (Micro)', 'KNN_Precision_Micro.png')\n",
    "save_plot(results['recall_micro'], 'Recall (Micro)', 'Recall (Micro)', 'KNN_Recall_Micro.png')\n",
    "save_plot(results['f1_micro'], 'F1 Score (Micro)', 'F1 Score (Micro)', 'KNN_F1_Micro.png')\n",
    "save_plot(results['precision_macro'], 'Precision (Macro)', 'Precision (Macro)', 'KNN_Precision_Macro.png')\n",
    "save_plot(results['recall_macro'], 'Recall (Macro)', 'Recall (Macro)', 'KNN_Recall_Macro.png')\n",
    "save_plot(results['f1_macro'], 'F1 Score (Macro)', 'F1 Score (Macro)', 'KNN_F1_Macro.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present Multiple Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k(knn, X, k=3, n_neighbors=50):\n",
    "    neighbors = knn.kneighbors(X, n_neighbors=n_neighbors, return_distance=False)\n",
    "\n",
    "    top_k_predictions = []\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_labels = y_train.iloc[neighbor]\n",
    "        top_k = neighbor_labels.value_counts().head(k).index.tolist()\n",
    "        top_k_predictions.append(top_k)\n",
    "    \n",
    "    print(\"Top k predictions: \", top_k_predictions)\n",
    "\n",
    "    return top_k_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_genres = 4\n",
    "top_k_predictions = predict_top_k(knn, X_test_scaled, k=k_genres)\n",
    "\n",
    "genre_list = []\n",
    "\n",
    "correct_count = 0\n",
    "for i, top_k in enumerate(top_k_predictions):\n",
    "    genre_list.append(y_test.iloc[i])\n",
    "    if y_test.iloc[i] in top_k:\n",
    "        correct_count += 1\n",
    "accuracy_top_k = correct_count / len(y_test)\n",
    "\n",
    "print(\"Genre List: \", genre_list)\n",
    "\n",
    "print(f\"Top {k_genres} Accuracy: {accuracy_top_k}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
