{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks = pd.read_csv(\"./data/cleaned_data_mil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data = tracks.copy()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "track_data['genre'] = encoder.fit_transform(track_data['genre'])\n",
    "\n",
    "features = track_data[['popularity', 'duration_ms', 'danceability', 'energy', 'key', 'loudness', 'mode', \n",
    "                       'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', \n",
    "                       'time_signature']]\n",
    "target = track_data['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=22)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "with open('./data/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('./data/encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_list = [20, 50, 60, 70, 100]\n",
    "\n",
    "results = {\n",
    "    'n_neighbors': [],\n",
    "    'accuracy': [],\n",
    "    'precision_micro': [],\n",
    "    'recall_micro': [],\n",
    "    'f1_micro': [],\n",
    "    'precision_macro': [],\n",
    "    'recall_macro': [],\n",
    "    'f1_macro': [],\n",
    "    'time': []\n",
    "}\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_n_neighbors = 0\n",
    "\n",
    "for n in n_neighbors_list:\n",
    "    start_time = time.time()\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    \n",
    "    results['n_neighbors'].append(n)\n",
    "    results['accuracy'].append(accuracy)\n",
    "    results['precision_micro'].append(precision_micro)\n",
    "    results['recall_micro'].append(recall_micro)\n",
    "    results['f1_micro'].append(f1_micro)\n",
    "    results['precision_macro'].append(precision_macro)\n",
    "    results['recall_macro'].append(recall_macro)\n",
    "    results['f1_macro'].append(f1_macro)\n",
    "    results['time'].append(end_time - start_time)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = knn\n",
    "        best_n_neighbors = n\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(f\"The best model with n_neighbors={best_n_neighbors} and accuracy={best_accuracy} has been saved as 'best_knn_model.pkl'.\")\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axs[0, 0].plot(results['n_neighbors'], results['accuracy'], marker='o')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 0].set_xlabel('n_neighbors')\n",
    "axs[0, 0].set_ylabel('Accuracy')\n",
    "\n",
    "axs[0, 1].plot(results['n_neighbors'], results['precision_micro'], marker='o')\n",
    "axs[0, 1].set_title('Precision (Micro)')\n",
    "axs[0, 1].set_xlabel('n_neighbors')\n",
    "axs[0, 1].set_ylabel('Precision (Micro)')\n",
    "\n",
    "axs[0, 2].plot(results['n_neighbors'], results['recall_micro'], marker='o')\n",
    "axs[0, 2].set_title('Recall (Micro)')\n",
    "axs[0, 2].set_xlabel('n_neighbors')\n",
    "axs[0, 2].set_ylabel('Recall (Micro)')\n",
    "\n",
    "axs[1, 0].plot(results['n_neighbors'], results['f1_micro'], marker='o')\n",
    "axs[1, 0].set_title('F1 Score (Micro)')\n",
    "axs[1, 0].set_xlabel('n_neighbors')\n",
    "axs[1, 0].set_ylabel('F1 Score (Micro)')\n",
    "\n",
    "axs[1, 1].plot(results['n_neighbors'], results['precision_macro'], marker='o')\n",
    "axs[1, 1].set_title('Precision (Macro)')\n",
    "axs[1, 1].set_xlabel('n_neighbors')\n",
    "axs[1, 1].set_ylabel('Precision (Macro)')\n",
    "\n",
    "axs[1, 2].plot(results['n_neighbors'], results['recall_macro'], marker='o')\n",
    "axs[1, 2].set_title('Recall (Macro)')\n",
    "axs[1, 2].set_xlabel('n_neighbors')\n",
    "axs[1, 2].set_ylabel('Recall (Macro)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN THIS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/best_knn_model_mil.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "\n",
    "\n",
    "results_df.to_csv('./result/KNN_Model_Evaluation_Results.txt', sep='\\t', index=False)\n",
    "\n",
    "def save_plot(y_values, title, ylabel, filename):\n",
    "    plt.figure()\n",
    "    plt.plot(results['n_neighbors'], y_values, marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('n_neighbors')\n",
    "    \n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'./result/{filename}')\n",
    "    plt.close()\n",
    "\n",
    "save_plot(results['accuracy'], 'Accuracy', 'Accuracy', 'KNN_Accuracy.png')\n",
    "save_plot(results['precision_micro'], 'Precision (Micro)', 'Precision (Micro)', 'KNN_Precision_Micro.png')\n",
    "save_plot(results['recall_micro'], 'Recall (Micro)', 'Recall (Micro)', 'KNN_Recall_Micro.png')\n",
    "save_plot(results['f1_micro'], 'F1 Score (Micro)', 'F1 Score (Micro)', 'KNN_F1_Micro.png')\n",
    "save_plot(results['precision_macro'], 'Precision (Macro)', 'Precision (Macro)', 'KNN_Precision_Macro.png')\n",
    "save_plot(results['recall_macro'], 'Recall (Macro)', 'Recall (Macro)', 'KNN_Recall_Macro.png')\n",
    "save_plot(results['f1_macro'], 'F1 Score (Macro)', 'F1 Score (Macro)', 'KNN_F1_Macro.png')\n",
    "save_plot(results['time'], 'Time', 'Time (s)', 'KNN_Time.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|:--------------:|:------------------:|:------------------:|\n",
    "| Actual Positive| True Positive (TP) | False Negative (FN)|\n",
    "| Actual Negative| False Positive (FP)| True Negative (TN) |\n",
    "\n",
    "For multi-class classification, this concept extends to multiple classes, but the fundamental principles remain the same.\n",
    "\n",
    "#### Accuracy\n",
    "- **Definition**: The proportion of correctly classified instances out of the total instances.\n",
    "- **Formula**: $ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $\n",
    "- **Interpretation**: It gives an overall effectiveness of the model, but can be misleading if the data is imbalanced.\n",
    "\n",
    "#### Precision\n",
    "- **Definition**: The proportion of true positive predictions among all positive predictions.\n",
    "- **Formula**: $ \\text{Precision} = \\frac{TP}{TP + FP} $\n",
    "- **Interpretation**: High precision indicates a low false positive rate.\n",
    "\n",
    "#### Recall (Sensitivity)\n",
    "- **Definition**: The proportion of true positive predictions among all actual positives.\n",
    "- **Formula**: $ \\text{Recall} = \\frac{TP}{TP + FN} $\n",
    "- **Interpretation**: High recall indicates a low false negative rate.\n",
    "\n",
    "#### F1 Score\n",
    "- **Definition**: The harmonic mean of precision and recall.\n",
    "- **Formula**: $ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $\n",
    "- **Interpretation**: It balances precision and recall, useful when you need to account for both false positives and false negatives.\n",
    "\n",
    "#### Micro vs. Macro Averaging\n",
    "- **Micro-Averaging**: Aggregates the contributions of all classes to compute the average metric. It is calculated globally by counting the total true positives, false negatives, and false positives.\n",
    "  - **Formula**:\n",
    "  $$ \\text{Accuracy}_\\text{micro} = \\frac{\\sum_i TP_i + TN_i}{\\text{num of datapoints}} $$\n",
    "- **Macro-Averaging**: Computes the metric independently for each class and then takes the average. This treats all classes equally, regardless of their size.\n",
    "  - **Formula**:\n",
    "  $$ \\text{Accuracy}_\\text{macro} = \\frac{\\sum_i \\text{Accuracy}_{\\text{class}_i}}{\\text{num of classes}} $$\n",
    "\n",
    "### Model Evaluation Based on Results\n",
    "\n",
    "The evaluation of the KNN model with different values of `n_neighbors` is based on accuracy, precision, recall, and F1 scores. Here are the results:\n",
    "\n",
    "| n_neighbors | accuracy | precision_micro | recall_micro | f1_micro | precision_macro | recall_macro | f1_macro | time_taken |\n",
    "|-------------|----------|-----------------|--------------|----------|-----------------|--------------|----------|------------|\n",
    "| 20          | 0.5301   | 0.5301          | 0.5301       | 0.5301   | 0.5266          | 0.5240       | 0.5233   | 160.06     |\n",
    "| 50          | 0.5335   | 0.5335          | 0.5335       | 0.5335   | 0.5315          | 0.5235       | 0.5249   | 186.72     |\n",
    "| 60          | 0.5340   | 0.5340          | 0.5340       | 0.5340   | 0.5323          | 0.5231       | 0.5250   | 200.10     |\n",
    "| 70          | 0.5335   | 0.5335          | 0.5335       | 0.5335   | 0.5316          | 0.5218       | 0.5237   | 207.17     |\n",
    "| 100         | 0.5333   | 0.5333          | 0.5333       | 0.5333   | 0.5319          | 0.5197       | 0.5224   | 216.07     |\n",
    "\n",
    "### Analysis\n",
    "\n",
    "1. **Accuracy**:\n",
    "   - The accuracy varies from 0.5301 (k=20) to 0.5340 (k=60). This small range of variation (approximately 0.004) suggests that the model's performance is relatively stable across different values of `n_neighbors`.\n",
    "\n",
    "2. **Precision (Micro)**:\n",
    "   - The micro-averaged precision follows the same trend as accuracy, ranging from 0.5301 to 0.5340. This indicates consistent performance in identifying true positives among predicted positives.\n",
    "\n",
    "3. **Recall (Micro)**:\n",
    "   - The micro-averaged recall also follows the same trend as accuracy, reflecting the model's ability to correctly identify actual positives.\n",
    "\n",
    "4. **F1 Score (Micro)**:\n",
    "   - The micro-averaged F1 score mirrors the precision and recall values, balancing the trade-off between precision and recall.\n",
    "\n",
    "5. **Precision (Macro)**:\n",
    "   - The macro-averaged precision ranges from 0.5266 (k=20) to 0.5323 (k=60). This suggests that the model's precision across different classes is relatively consistent.\n",
    "\n",
    "6. **Recall (Macro)**:\n",
    "   - The macro-averaged recall ranges from 0.5197 (k=100) to 0.5240 (k=20). This indicates that the model's recall across different classes does not vary significantly.\n",
    "\n",
    "7. **F1 Score (Macro)**:\n",
    "   - The macro-averaged F1 score ranges from 0.5224 (k=100) to 0.5250 (k=60), reflecting a balanced performance across different classes.\n",
    "\n",
    "8. **Time Taken**:\n",
    "   - The time taken ranges from 160.06 seconds (k=20) to 216.07 seconds (k=100). This indicates that as the value of `n_neighbors` increases, the computation time also increases.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The evaluation shows that the KNN model's performance is relatively stable across different values of `n_neighbors`. The differences in accuracy, precision, recall, and F1 score are minimal, with the highest performance metrics observed at `n_neighbors = 60`. However, the computation time increases as the value of `n_neighbors` increases. Given these results, one might prioritize other factors such as computational efficiency or simplicity when choosing the optimal `n_neighbors`. \n",
    "\n",
    "Overall, the KNN model demonstrates consistent performance, indicating its robustness for this multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('knn_model.pkl', 'rb') as f:\n",
    "    knn_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Present Multiple Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top_k(knn, X, k=3, n_neighbors=best_n_neighbors):\n",
    "    neighbors = knn.kneighbors(X, n_neighbors=n_neighbors, return_distance=False)\n",
    "\n",
    "    top_k_predictions = []\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_labels = y_train.iloc[neighbor]\n",
    "        top_k = neighbor_labels.value_counts().head(k).index.tolist()\n",
    "        top_k_predictions.append(top_k)\n",
    "    \n",
    "    print(\"Top k predictions: \", top_k_predictions)\n",
    "\n",
    "    return top_k_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(np.unique(y_train)) # 9\n",
    "\n",
    "k_genres_list = range(2, N)\n",
    "accuracy_top_k_list = []\n",
    "weighted_accuracy_list = []\n",
    "\n",
    "for k_genres in k_genres_list:\n",
    "    top_k_predictions = predict_top_k(knn_loaded, X_test_scaled, k=k_genres)\n",
    "\n",
    "    correct_count = 0\n",
    "    for i, top_k in enumerate(top_k_predictions):\n",
    "        if y_test.iloc[i] in top_k:\n",
    "            correct_count += 1\n",
    "    accuracy_top_k = correct_count / len(y_test)\n",
    "    accuracy_top_k_list.append(accuracy_top_k)\n",
    "\n",
    "    weighted_accuracy = accuracy_top_k * np.exp(-k_genres / N)\n",
    "    weighted_accuracy_list.append(weighted_accuracy)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_genres_list, accuracy_top_k_list, marker='o')\n",
    "plt.title('Top K Accuracy')\n",
    "plt.xlabel('k_genres')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_genres_list, weighted_accuracy_list, marker='o')\n",
    "plt.title('Weighted Top K Accuracy')\n",
    "plt.xlabel('k_genres')\n",
    "plt.ylabel('Weighted Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
