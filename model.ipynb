{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carson\\AppData\\Local\\Temp\\ipykernel_12644\\2437565312.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from IPython.display import display\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'min_samples_split': 5, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "tracks = pd.read_csv(\"cleaned_data_f.csv\")\n",
    "\n",
    "features = tracks[['danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
    "                    'acousticness', 'instrumentalness', 'valence', 'tempo']]\n",
    "target = tracks['track_genre']\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "model = RandomForestClassifier()\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(features, target)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47935103244837757\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    alt-rock       0.38      0.34      0.36       365\n",
      "   classical       0.82      0.81      0.82       155\n",
      "     country       0.34      0.11      0.17       108\n",
      "         edm       0.65      0.73      0.69       637\n",
      "     hip-hop       0.39      0.24      0.30       134\n",
      "        jazz       0.50      0.16      0.24        44\n",
      "       latin       0.48      0.32      0.38       103\n",
      "         pop       0.31      0.35      0.33       400\n",
      "   punk-rock       0.40      0.47      0.43       474\n",
      "        rock       0.45      0.47      0.46       292\n",
      "\n",
      "    accuracy                           0.48      2712\n",
      "   macro avg       0.47      0.40      0.42      2712\n",
      "weighted avg       0.47      0.48      0.47      2712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "tracks = pd.read_csv(\"cleaned_data_f.csv\")\n",
    "\n",
    "features = tracks[['danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
    "                    'acousticness', 'instrumentalness', 'valence', 'tempo']]\n",
    "target = tracks['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model using the Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators = 300, min_samples_split = 5)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4476401179941003\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    alt-rock       0.35      0.34      0.35       365\n",
      "   classical       0.84      0.80      0.82       155\n",
      "     country       0.30      0.15      0.20       108\n",
      "         edm       0.65      0.69      0.67       637\n",
      "     hip-hop       0.28      0.19      0.22       134\n",
      "        jazz       0.45      0.23      0.30        44\n",
      "       latin       0.42      0.28      0.34       103\n",
      "         pop       0.28      0.32      0.30       400\n",
      "   punk-rock       0.36      0.42      0.39       474\n",
      "        rock       0.40      0.41      0.41       292\n",
      "\n",
      "    accuracy                           0.45      2712\n",
      "   macro avg       0.43      0.38      0.40      2712\n",
      "weighted avg       0.44      0.45      0.44      2712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Load the data\n",
    "tracks = pd.read_csv(\"cleaned_data_f.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = tracks[['danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
    "                    'acousticness', 'instrumentalness', 'valence', 'tempo']]\n",
    "target = tracks['genre']\n",
    "\n",
    "# Encode the target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoded = label_encoder.fit_transform(target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the model using the XGBoost Classifier\n",
    "model = xgb.XGBClassifier(n_estimators=300, use_label_encoder=True, eval_metric='mlogloss')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Decode the predictions back to original labels\n",
    "predictions_decoded = label_encoder.inverse_transform(predictions)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_decoded, predictions_decoded))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_decoded, predictions_decoded, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 5614: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(f)\n\u001b[1;32m---> 14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create an instance of the encoder\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carson\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\csv.py:116\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m--> 116\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mline_num\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# unlike the basic reader, we prefer not to return blanks,\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# because we will typically wind up with a dict full of None\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# values\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carson\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 5614: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "tracks = pd.read_csv(\"cleaned_data_f.csv\")\n",
    "\n",
    "# Create an instance of the encoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "\n",
    "features = tracks[['danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
    "                    'acousticness', 'instrumentalness', 'valence', 'tempo']]\n",
    "target = tracks['genre']\n",
    "\n",
    "# Fit the encoder to each feature in rel_feats and transform the data\n",
    "X_encoded = []\n",
    "for feat in features:\n",
    "    feat_data = np.array(feat for row in tracks)  # Extract feature data\n",
    "    feat_encoded = encoder.fit_transform(feat_data)  # Fit and transform the encoder\n",
    "    feat_encoded = feat_encoded.toarray()  # Convert to numpy array\n",
    "    X_encoded.append(feat_encoded)\n",
    "# Concatenate the encoded features horizontally\n",
    "X_encoded = np.concatenate(X_encoded, axis=1)\n",
    "\n",
    "print(\"X_encoded:\", X_encoded)\n",
    "\n",
    "# Extract target labels\n",
    "y = np.array([d['Level'] for d in tracks])\n",
    "\n",
    "# Encode the target labels to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "target_encoded = label_encoder.fit_transform(target)\n",
    "\n",
    "\n",
    "# Split the mapped dataset into train and test sets [80:20]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, target_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test_scaled)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
